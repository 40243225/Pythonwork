# -*- coding: utf8 -*-
# coding: utf8

from pyspark import SparkContext
from pyspark import  SparkConf
from pyspark.sql import Row
from pyspark.sql import SQLContext
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import  Tokenizer
from pyspark.mllib.feature import HashingTF, IDF
from pyspark.ml.feature import CountVectorizer
import sys
import time

def SetLogger( sc ):
    logger = sc._jvm.org.apache.log4j
    logger.LogManager.getLogger("org"). setLevel( logger.Level.ERROR )
    logger.LogManager.getLogger("akka").setLevel( logger.Level.ERROR )
    logger.LogManager.getRootLogger().setLevel(logger.Level.ERROR)
    
def SetPath(sc):
    global Path
    if sc.master[0:5]=="local" :
        Path="file:/home/hduser/pythonwork/PythonProject/"
    else:   
        Path="hdfs://master:9000/user/hduser/"
       
def  CreateSparkContext():
    sparkConf =SparkConf().setAppName("mlTF-IDF").set("spark.ui.showConsoleProgress","false")
    sc = SparkContext(conf = sparkConf)
    print("master="+sc.master)
    SetLogger(sc)
    SetPath(sc)
    return (sc)
def getmax(ind,val,count):
    result=list()
    for i in range(0,count):    
        maxval=max(val)
        maxindex=val.index(maxval)
        result.append(ind[maxindex])
        ind.remove(ind[maxindex])
        val.remove(val[maxindex])
    return result
if __name__ == "__main__":
    start = time.time()
    print("開始執行TF-IDF")
    
    sc=CreateSparkContext()
    reload(sys)
    sys.setdefaultencoding('utf-8')
    sqlContext=SQLContext(sc)
    
    documents = sc.textFile("data/food/food.txt").map(lambda line: (line.split("|")))
    print ("載入字詞完成")                                                                                                                             
    sentence=documents.map(lambda p:Row(id=int(p[0]),title=(p[1]),context=(p[2]),sentence=(p[3].split(" ")))) 
    sentence_df=sqlContext.createDataFrame(sentence) 
    vectorizer = CountVectorizer(inputCol="sentence", outputCol="features").fit(sentence_df)
    print ("Data frame OK")
    sen=sentence_df.select("sentence").collect()
    sen_list=list()
    for i in sen:
        sen_list.append(i.sentence)
    documents= sc.parallelize(sen_list)
    hashingTF = HashingTF()
    print ("documents index OK")
    word_list=list()
    vocabulary=vectorizer.vocabulary
    for i in range(len(vocabulary)):
        indx=hashingTF.indexOf(vocabulary[i])
        word_list.append((indx,vocabulary[i]))
    print ("Word index OK")
    wordindex=sc.parallelize(word_list)
    wordMap=wordindex.collectAsMap()
    bcwordindex=sc.broadcast(wordMap)
    
    tf=hashingTF.transform(documents)
    idf = IDF().fit(tf)
    tfidf = idf.transform(tf)
    # spark.mllib's IDF implementation provides an option for ignoring terms
    # which occur in less than a minimum number of documents.
    # In such cases, the IDF for these terms is set to 0.
    # This feature can be used by passing the minDocFreq value to the IDF constructor.
    '''
    idfIgnore = IDF(minDocFreq=2).fit(tf)
    tfidfIgnore = idfIgnore.transform(tf)
    '''
    print("TF-IDF完成")
    print("tfidf:")
    i=1
    keywordlist=list()
    for each in tfidf.collect():
        ind=each.indices.tolist()
        val=each.values.tolist()
        if len(ind)>=5:   
            result=getmax(ind,val,5)
        else:
            result=getmax(ind,val,len(ind))
        bclist=list()
        for i in result:
            bclist.append(bcwordindex.value[i])
        keywordlist.append(bclist)     
    print ("開始匯入data frmae")    
    output=Row(keywords=keywordlist)
    print output
    '''
    print("tfidfIgnore:")
    for each in tfidfIgnore.collect():
        ind=each.indices.tolist()
        val=each.values.tolist()
        result=getmax(ind,val,5)
        for i in result:
            print bcwordindex.value[i]
    end = time.time()
    elapsed = end - start
    print "Time taken: ", elapsed, "seconds."
    sc.stop()
    '''
   