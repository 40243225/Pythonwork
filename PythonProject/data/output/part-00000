(u'', 39)
(u' see [Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse)', 1)
(u'and Spark Streaming for stream processing.', 1)
(u' GraphX for graph processing', 1)
(u'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported', 1)
(u'And run the following command', 1)
(u'for detailed guidance on building for a particular distribution of Hadoop', 1)
(u'    ./bin/run-example SparkPi', 1)
(u'# Apache Spark', 1)
(u'can also use an abbreviated class name if the class is in the `examples`', 1)
(u'## Running Tests', 1)
(u'## Example Programs', 1)
(u'## Online Documentation', 1)
(u'<http://spark.apache.org/>', 1)
(u' on the [project web page](http://spark.apache.org/documentation.html)', 1)
(u' at', 1)
(u'## Interactive Scala Shell', 1)
(u'examples to a cluster. This can be a mesos:// or spark:// URL', 1)
(u'package. For instance:', 1)
(u'    MASTER=spark://host:7077 ./bin/run-example SparkPi', 1)
(u'Spark also comes with several sample programs in the `examples` directory.', 1)
(u'Spark is a fast and general cluster computing system for Big Data. It provides', 1)
(u' run:', 1)
(u'Testing first requires [building Spark](#building-spark). Once Spark is built', 1)
(u'"yarn" to run on YARN', 1)
(u'["Specifying the Hadoop Version"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 1)
(u'## Configuration', 1)
(u'You can find the latest Spark documentation', 1)
(u'MLlib for machine learning', 1)
(u'    >>> sc.parallelize(range(1000)).count()', 1)
(u'The easiest way to start using Spark is through the Scala shell:', 1)
(u'[run tests for a module', 1)
(u'## Building Spark', 1)
(u'    ./bin/spark-shell', 1)
(u'and [IntelliJ](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ).', 1)
(u'    ./bin/pyspark', 1)
(u' use `./bin/run-example <class> [params]`. For example:', 1)
